# -*- coding: utf-8 -*-
"""LLama Code Fine Tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hK5QAsNtCnQEEFltTFyOCLNG5U0lg81U
"""

!pip install -q transformers datasets accelerate trl peft bitsandbytes unsloth evaluate rouge_score sacrebleu

import os
import re
import subprocess
import tempfile
from collections import Counter

import torch
from huggingface_hub import login
from google.colab import userdata, drive
from datasets import load_from_disk, load_dataset
from transformers import TrainingArguments, Trainer
from unsloth import FastLanguageModel

# -----------------------------------------------------------------------------
# 1. AUTH & DRIVE SETâ€‘UP
# -----------------------------------------------------------------------------
login(token=userdata.get("HF_TOKEN"))
drive.mount("/content/drive")

DRIVE_BASE = "/content/drive/MyDrive"
OUTPUT_DIR = f"{DRIVE_BASE}/llama-opencode"
TOKENIZED_TRAIN_PATH = f"{DRIVE_BASE}/opencode_tokenized_train"
TOKENIZED_TEST_PATH = f"{DRIVE_BASE}/opencode_tokenized_test"

# -----------------------------------------------------------------------------
# 2. HYPERâ€‘PARAMETERS (EDIT IN ONE SPOT)
# -----------------------------------------------------------------------------
MAX_LENGTH = 2048
BASE_MODEL = "unsloth/llama-2-13b-bnb-4bit"
EPOCHS      = 1
LR          = 2e-4

# LoRA
LORA_R          = 64
LORA_ALPHA      = 64
LORA_DROPOUT    = 0.1

# Generation / evaluation
GEN_K           = 10             # candidates per problem (pass@k)
TEMPERATURE     = 0.7
TOP_P           = 0.95
MAX_NEW_TOKENS  = 256

KEEP_COLS         = ["input_ids", "attention_mask", "labels"]  # only these go to Trainer

# -----------------------------------------------------------------------------
# 3. HELPERS
# -----------------------------------------------------------------------------

def get_latest_checkpoint(path: str):
    if not os.path.exists(path):
        return None
    ckpts = [d for d in os.listdir(path) if d.startswith("checkpoint-")]
    if not ckpts:
        return None
    latest = sorted(ckpts, key=lambda x: int(x.split("-")[-1]))[-1]
    return os.path.join(path, latest)

RESUME_FROM = get_latest_checkpoint(OUTPUT_DIR)
print("Resuming from:", RESUME_FROM or "scratch")

# -----------------------------------------------------------------------------
# 4. MODEL + TOKENISER
# -----------------------------------------------------------------------------
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = RESUME_FROM or BASE_MODEL,
    max_seq_length = MAX_LENGTH,
    dtype = torch.float16,
    load_in_4bit = True,
)

tokenizer.pad_token = tokenizer.eos_token

model = FastLanguageModel.get_peft_model(
    model,
    r                  = LORA_R,
    lora_alpha         = LORA_ALPHA,
    lora_dropout       = LORA_DROPOUT,
    bias               = "none",
    use_gradient_checkpointing = True,
    random_state       = 42,
    use_rslora         = False,
    loftq_config       = None,
)

# -----------------------------------------------------------------------------
# 5. DATASET LOADING / TOKENISATION (with FIM option)
# -----------------------------------------------------------------------------

def is_within_length(example):
    combined = f"<problem>\n{example['input'].strip()}\n<solution>\n{example['solution'].strip()}"
    return len(tokenizer(combined).input_ids) <= MAX_LENGTH


def tokenize(batch):
    full = [f"<problem>\n{i.strip()}\n<solution>\n{s.strip()}" for i, s in zip(batch["input"], batch["solution"])]
    toks = tokenizer(full, truncation=True, padding="max_length", max_length=MAX_LENGTH)
    toks["labels"] = toks["input_ids"].copy()
    return toks

if os.path.exists(TOKENIZED_TRAIN_PATH) and os.path.exists(TOKENIZED_TEST_PATH):
    print("Loading tokenised splits â€¦")
    tokenized_train = load_from_disk(TOKENIZED_TRAIN_PATH)
    tokenized_test  = load_from_disk(TOKENIZED_TEST_PATH)
else:
    print("Tokenising raw OpenCodeReasoning split â€¦")
    ds = load_dataset("nvidia/OpenCodeReasoning", "split_0")["split_0"]
    raw_split = ds.train_test_split(test_size=0.01)
    filtered_train = raw_split["train"].filter(is_within_length)
    filtered_test  = raw_split["test"].filter(is_within_length)
    tokenized_train = filtered_train.map(tokenize, batched=True)
    tokenized_test  = filtered_test .map(tokenize, batched=True)
    tokenized_train.save_to_disk(TOKENIZED_TRAIN_PATH)
    tokenized_test .save_to_disk(TOKENIZED_TEST_PATH)

# ðŸ”‘  Drop non-tensor columns to stop Trainer ValueError
for ds_name, ds in {"train": tokenized_train, "test": tokenized_test}.items():
    drop_cols = [c for c in ds.column_names if c not in KEEP_COLS]
    if drop_cols:
        print(f"â€¢ Removing non-tensor columns from {ds_name}:", drop_cols)
        if ds_name == "train":
            tokenized_train = ds.remove_columns(drop_cols)
        else:
            tokenized_test  = ds.remove_columns(drop_cols)

print("Example tokenised row:", tokenized_train[0])

# Clamp to a small subset if debugging
# tokenized_train = tokenized_train.select(range(25000))

# -----------------------------------------------------------------------------
# 6. TRAINING ARGS
# -----------------------------------------------------------------------------
training_args = TrainingArguments(
    output_dir              = OUTPUT_DIR,
    num_train_epochs        = EPOCHS,
    per_device_train_batch_size = 8,
    per_device_eval_batch_size  = 8,
    gradient_accumulation_steps = 4,
    learning_rate           = LR,
    lr_scheduler_type       = "cosine",
    warmup_ratio            = 0.03,
    logging_steps           = 50,
    save_strategy           = "steps",
    save_steps              = 100,
    save_total_limit        = 2,
    fp16                    = True,
    report_to               = "none",
    remove_unused_columns   = False,
)

trainer = Trainer(
    model          = model,
    args           = training_args,
    train_dataset  = tokenized_train,
    eval_dataset   = None,
    tokenizer      = tokenizer,
)

trainer.train(resume_from_checkpoint = RESUME_FROM)
model.save_pretrained(f"{DRIVE_BASE}/llama-finetuned")

# -----------------------------------------------------------------------------
# 7. EVALUATION â€“ PASS@k WITH SAMPLING
# -----------------------------------------------------------------------------

ev_ds = load_dataset("nvidia/OpenCodeReasoning", "split_0")["split_0"].train_test_split(test_size=0.01)["test"].select(range(100))
cf_ds = ev_ds.filter(lambda e: e["source"] == "codeforces")

finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(
    model_name      = f"{DRIVE_BASE}/llama-finetuned",
    max_seq_length  = MAX_LENGTH,
    dtype           = torch.float16,
    load_in_4bit    = True,
)
finetuned_model.eval(); finetuned_tokenizer.pad_token = finetuned_tokenizer.eos_token


def format_prompt(problem: str) -> str:
    return (
        "<|system|>\nYou are an expert competitiveâ€‘programming assistant.\n<|user|>\n### Problem\n" + problem.strip() +
        "\n<|assistant|>\n```python\n"
    )


def generate_k(model, tokenizer, prompt: str, k: int = GEN_K):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            do_sample=True,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            max_new_tokens=MAX_NEW_TOKENS,
            num_return_sequences=k,
            eos_token_id=tokenizer.eos_token_id,
        )
    dec = tokenizer.batch_decode(out, skip_special_tokens=True)
    # keep only code after first python fence
    return [seg.split("```python\n",1)[-1] for seg in dec]


def execute_code(code: str, inp: str, timeout: int = 5):
    with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
        f.write(code)
        path = f.name
    try:
        p = subprocess.run(["python3", path], input=inp, text=True, capture_output=True, timeout=timeout)
        return p.stdout.strip(), p.stderr.strip(), p.returncode
    except subprocess.TimeoutExpired:
        return "", "Timeout", -1
    finally:
        os.remove(path)


def normalise(s: str):
    return "\n".join(l.strip().lower() for l in s.splitlines() if l.strip())


pass_any = 0
reason_counter = Counter()

for ex in cf_ds:
    prompt = ex["input"]
    inp    = re.search(r"Examples.*?Input\s*\n(.*?)\n+Output", prompt, re.S).group(1).strip()
    exp    = re.search(r"Output\s*\n(.*?)($|\n{2,})", prompt, re.S).group(1).strip()

    candidates = generate_k(finetuned_model, finetuned_tokenizer, format_prompt(prompt))
    solved = False
    for code in candidates:
        out, err, rc = execute_code(code, inp)
        if rc == 0 and normalise(out) == normalise(exp):
            solved = True
            break
    if solved:
        pass_any += 1
    else:
        reason_counter.update(["fail"])

print(f"\nPASS@{GEN_K}: {pass_any}/{len(cf_ds)} = {pass_any/len(cf_ds):.2%}")